{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pathlib\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# honeyquest path hack to make module imports work\n",
    "__package__ = \"honeyquest\"\n",
    "modulepath = pathlib.Path.cwd().joinpath(\"../../../\").resolve().as_posix()\n",
    "if modulepath not in sys.path:\n",
    "    sys.path.append(modulepath)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from honeyquest.data.ops import (\n",
    "    anonymizing,\n",
    "    cleaning,\n",
    "    counting,\n",
    "    loading,\n",
    "    processing,\n",
    "    transforming,\n",
    ")\n",
    "from honeyquest.data.ops.aspects.defensiveness import (\n",
    "    comnpute_defensive_rank_preference_table,\n",
    "    compute_defensive_distraction_table,\n",
    ")\n",
    "from honeyquest.data.ops.aspects.enticingness import (\n",
    "    compute_enticingness_confusion_matrix,\n",
    "    compute_enticingness_table,\n",
    ")\n",
    "from honeyquest.data.ops.aspects.marking import (\n",
    "    compute_mark_distribution_by_line_annotation_length,\n",
    "    compute_mark_distribution_by_mark_completeness,\n",
    "    compute_mark_distribution_by_mark_variant,\n",
    "    compute_mark_ranking,\n",
    "    compute_mark_statistics,\n",
    ")\n",
    "from honeyquest.data.ops.generation.latex.tables.defensiveness import (\n",
    "    generate_defensive_distraction_latex_table,\n",
    ")\n",
    "from honeyquest.data.ops.generation.latex.tables.enticingness import (\n",
    "    generate_enticingness_latex_table,\n",
    ")\n",
    "from honeyquest.data.ops.generation.latex.tables.marking import (\n",
    "    generate_mark_ranking_latex_table,\n",
    ")\n",
    "from honeyquest.data.ops.generation.latex.variables import store_latex_variables\n",
    "from honeyquest.data.ops.visuals.activity import plot_user_activity\n",
    "from honeyquest.data.ops.visuals.answers import (\n",
    "    plot_number_of_queries_answered,\n",
    "    plot_query_rating,\n",
    "    plot_query_response_time,\n",
    ")\n",
    "from honeyquest.data.ops.visuals.datasets import (\n",
    "    plot_query_label_distribution_per_bucket,\n",
    ")\n",
    "from honeyquest.data.ops.visuals.demographics import (\n",
    "    plot_favorite_colors,\n",
    "    plot_job_roles,\n",
    "    plot_skill_levels,\n",
    "    plot_years_of_experience,\n",
    ")\n",
    "from honeyquest.data.util import jupyter\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.float_format\", \"{:.6f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CONFIG = {**dotenv_values(\".env.shared\"), **dotenv_values(\".env.local\")}\n",
    "\n",
    "QUERY_DATA_PATH = Path.cwd().joinpath(NB_CONFIG[\"QUERY_DATA_PATH\"]).resolve().as_posix()\n",
    "QUERY_INDEX_PATH = Path.cwd().joinpath(NB_CONFIG[\"QUERY_INDEX_PATH\"]).resolve().as_posix()\n",
    "\n",
    "LOCAL_BASE = Path(NB_CONFIG.get(\"LOCAL_BASE\", \"\")).resolve().as_posix()\n",
    "LOCAL_PATHS = { key.split(\"__\")[1]: val for key, val in NB_CONFIG.items() if key.startswith(\"LOCAL_PATHS__\") }\n",
    "\n",
    "LIVE_URL = NB_CONFIG[\"LIVE_URL\"]\n",
    "LIVE_TOKENS = { key.split(\"__\")[1]: val for key, val in NB_CONFIG.items() if key.startswith(\"LIVE_TOKENS__\") }\n",
    "\n",
    "ANONYMIZE = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🚀 Data Loading and Cleaning**\n",
    "\n",
    "Throughout this notebook, we refer to the following identifiers:\n",
    "\n",
    "- `uid`: Globally-unique user ID\n",
    "- `eid`: Globally-unique experiment ID (there might be different deployments of Honeyquest)\n",
    "- `sid`: Session ID, i.e., just a timestamp when a user started a session (this is the `qid` in the API) - **this is not used!**\n",
    "- `rid`: ID for a response (that is composed of answer mars) that a user submitted to a query\n",
    "- `mid`: ID for an individual mark (part of a response) that a user placed on a query\n",
    "- `qid`: ID for a query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all queries and responses for all experiments\n",
    "QUERIES_DICT = loading.parse_all_queries(QUERY_DATA_PATH)\n",
    "BUCKETS_DICT = loading.parse_index_buckets(QUERY_INDEX_PATH)\n",
    "RESULTS_DICT = loading.load_experiments(LOCAL_BASE, LOCAL_PATHS, LIVE_URL, LIVE_TOKENS)\n",
    "\n",
    "# fully neglect responses from the tutorial\n",
    "cleaning.drop_tutorial(RESULTS_DICT, clean_profiles=True)\n",
    "\n",
    "# remove tutorial queries and inactive users with less than 8 warm-up responses (excluding the 8 tutorial queries)\n",
    "MIN_RESPONSES = 8\n",
    "NUM_DROPPED_USERS = cleaning.drop_inactive(RESULTS_DICT, min_responses=MIN_RESPONSES)\n",
    "\n",
    "# for the review submission, we perform a special anonymization\n",
    "if ANONYMIZE:\n",
    "    anonymized_qids = {\n",
    "        \"TR150.httpheaders.dynatrace_timecokpit-xhr\",\n",
    "        \"TR150.httpheaders.http-request-smuggling-clte\",\n",
    "        \"TR150.networkrequests.jira_dynatrace_org-trunc-02-insufficient-logging-monitoring\",\n",
    "        \"TR150.networkrequests.www_dynatrace_com-trunc\",\n",
    "        \"TR150.networkrequests.jira_dynatrace_org-trunc-01\",\n",
    "        \"TR150.networkrequests.jira_dynatrace_org-trunc-02\",\n",
    "        \"TR849.networkrequests.sahin.jira_dynatrace_org-trunc-02-inject-script-tag\",\n",
    "    }\n",
    "\n",
    "    anonymizing.anonymize_results(RESULTS_DICT, anonymized_qids)\n",
    "    QUERIES_DICT = anonymizing.anonymize_queries(QUERIES_DICT, anonymized_qids)\n",
    "\n",
    "merge_cr = [\"ea641b92-df3d-435b-b80e-fa40f797794c\", \"9d88637a-c260-4518-9fa0-19850e8526c6\"]\n",
    "merge_el = [\"09e3d8a2-431f-4370-89d2-e30e872d7828\", \"0b5952cf-38a8-4321-bc79-cf57387a3def\"]\n",
    "merge_gr = [\"0492c47c-5082-4b9b-82e3-a4286bb0db11\", \"fb4cd075-79c5-4d77-8233-015ca6dbbf34\"]\n",
    "merge_me = [\"89697b49-93a7-4dc4-9781-dcf05b58856d\", \"1105ba7d-e259-4470-a3d9-f9a9a3518ecd\"]\n",
    "\n",
    "# merge a few users together\n",
    "cleaning.merge_users(RESULTS_DICT, merge_cr)\n",
    "cleaning.merge_users(RESULTS_DICT, merge_el)\n",
    "cleaning.merge_users(RESULTS_DICT, merge_gr)\n",
    "cleaning.merge_users(RESULTS_DICT, merge_me)\n",
    "\n",
    "# drop duplicate responses (can occur through client-side retries)\n",
    "cleaning.drop_duplicate_responses(RESULTS_DICT)\n",
    "\n",
    "# filter out the pr0 experiment for now and merge ex3 into ctf1\n",
    "RESULTS_DICT = cleaning.filter_experiments(RESULTS_DICT, [\"ex1\", \"ex2\", \"ex3\", \"ctf1\"])\n",
    "RESULTS_DICT = cleaning.merge_experiments(RESULTS_DICT, [\"ctf1\", \"ex3\"])\n",
    "\n",
    "# flatten and postprocess the data\n",
    "QUERIES_DF = transforming.flatten_queries(QUERIES_DICT, drop_tutorial=True)\n",
    "MARKS_DF, USERS_DF = transforming.flatten_experiments(RESULTS_DICT)\n",
    "MARKS_DF, RESPONSES_DF = processing.postprocess_marks(MARKS_DF, QUERIES_DICT)\n",
    "\n",
    "# merge users with their answer activity\n",
    "ACTIVITY_DF = counting.get_user_activity(RESULTS_DICT, MARKS_DF)\n",
    "USERS_DF = transforming.merge_user_activity(USERS_DF, ACTIVITY_DF)\n",
    "\n",
    "# dataframes just with the ids for easy merges\n",
    "MARK_IDS = MARKS_DF[[\"eid\", \"uid\", \"rid\", \"qid\"]]\n",
    "RESPONSE_IDS = MARK_IDS.drop_duplicates().set_index(\"rid\")\n",
    "assert MARK_IDS.index.is_unique\n",
    "assert RESPONSE_IDS.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERVIEW_DF = counting.get_overview_counts(MARKS_DF)\n",
    "display(OVERVIEW_DF)\n",
    "\n",
    "plot_user_activity(MARKS_DF)\n",
    "plot_user_activity(MARKS_DF.query(\"answer_time > '2024-01-01T00:00Z'\"))\n",
    "\n",
    "display(USERS_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👤 **Main Figures** on user answers, demographics, and the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_of_queries_answered(\"./outputs/figures/fig1a.pdf\", RESULTS_DICT, display_df=False)\n",
    "plot_query_response_time(\"./outputs/figures/fig1b.pdf\", MARKS_DF, display_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_job_roles(\"./outputs/figures/fig2a.pdf\", USERS_DF, display_df=False)\n",
    "plot_skill_levels(\"./outputs/figures/fig2b.pdf\", USERS_DF, display_df=False)\n",
    "plot_years_of_experience(\"./outputs/figures/fig2c.pdf\", USERS_DF)\n",
    "plot_favorite_colors(\"./outputs/figures/fig2d.pdf\", USERS_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_query_label_distribution_per_bucket(\"./outputs/figures/fig3.pdf\", QUERIES_DICT, BUCKETS_DICT, display_df=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 **Aspect 1:** How good are humans in detecting deceptive and risky elements in our queries?\n",
    "\n",
    "This is an _answer-based analysis_, meaning we report statistics per query response / user answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT1_DF = compute_enticingness_table(RESPONSE_IDS, RESPONSES_DF, QUERIES_DF)\n",
    "display(ASPECT1_DF)\n",
    "\n",
    "CM_TRAP, CM_HACK = compute_enticingness_confusion_matrix(ASPECT1_DF)\n",
    "display(CM_TRAP)\n",
    "display(CM_HACK)\n",
    "\n",
    "# grap two series that just map honeywires and risk to their identifiers\n",
    "ID_DCPT_DF = ASPECT1_DF[[\"applied_honeywire\", \"identifier\"]].dropna().set_index(\"applied_honeywire\")\n",
    "ID_RISK_DF = ASPECT1_DF[[\"present_risk\", \"identifier\"]].dropna().set_index(\"present_risk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 **Aspect 2:** Are deceptive elements diverting an attackers interest away from real weaknesses and vulnerabilities in our queries?\n",
    "\n",
    "This is an _answer-based analysis_, meaning we report statistics per query response / user answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT2_DF = compute_defensive_distraction_table(RESPONSE_IDS, RESPONSES_DF, QUERIES_DF)\n",
    "display(ASPECT2_DF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 **Aspect 3:** Do attackers prefer to exploit deceptive elements before other elements in our queries?\n",
    "\n",
    "This is an _answer-based analysis_, meaning we report statistics per query response / user answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT3_DCPT_DF = comnpute_defensive_rank_preference_table(RESPONSE_IDS, RESPONSES_DF, QUERIES_DF, \"applied_honeywire\")\n",
    "display(ASPECT3_DCPT_DF)\n",
    "\n",
    "ASPECT3_RISK_DF = comnpute_defensive_rank_preference_table(RESPONSE_IDS, RESPONSES_DF, QUERIES_DF, \"present_risk\")\n",
    "display(ASPECT3_RISK_DF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 **Aspect 4:** How often where deceptive lines marked for hack or trap?\n",
    "\n",
    "This is a _mark-based analysis_, meaning we report statistics per marks, i.e., users might place multiple marks within a single answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT4_DFS = compute_mark_statistics(MARKS_DF, QUERIES_DF)\n",
    "ASPECT4_LINES, ASPECT4_DCPT, ASPECT4_RISK = ASPECT4_DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter.display_sortable_df(ASPECT4_DCPT, by=\"mrk_hack_on_dcpt\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter.display_sortable_df(ASPECT4_RISK, by=\"mrk_hack_on_risk\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter.display_sortable_df(ASPECT4_LINES, by=\"mrk_hack_on_dcpt\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 **Aspect 5:** What queries received the most marks?\n",
    "\n",
    "This is a _mark-based analysis_, meaning we report statistics per marks, i.e., users might place multiple marks within a single answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT5_DF = compute_mark_ranking(MARKS_DF)\n",
    "display(ASPECT5_DF.head(n=25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📋 **Aspect 6:** How are the different mark variants distributed?\n",
    "\n",
    "This is a _mark-based analysis_, meaning we report statistics per marks, i.e., users might place multiple marks within a single answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT6A_DF = compute_mark_distribution_by_mark_variant(RESPONSES_DF)\n",
    "display(ASPECT6A_DF)\n",
    "\n",
    "ASPECT6B_DF = compute_mark_distribution_by_mark_completeness(RESPONSES_DF)\n",
    "display(ASPECT6B_DF)\n",
    "\n",
    "ASPECT6C_DF = compute_mark_distribution_by_line_annotation_length(QUERIES_DF)\n",
    "display(ASPECT6C_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗃️ **Export:** Export CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./outputs/export\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "USERS_DF.drop(\"nickname\", axis=1).to_csv(\"./outputs/export/honeyquest_users.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC, lineterminator=\"\\n\")\n",
    "MARKS_DF.drop(\"mrk_query_type\", axis=1).to_csv(\"./outputs/export/honeyquest_marks.csv\", index=True, quoting=csv.QUOTE_NONNUMERIC, lineterminator=\"\\n\")\n",
    "RESPONSES_DF.to_csv(\"./outputs/export/honeyquest_responses.csv\", index=True, quoting=csv.QUOTE_NONNUMERIC, lineterminator=\"\\n\")\n",
    "QUERIES_DF.to_csv(\"./outputs/export/honeyquest_queries.csv\", index=True, quoting=csv.QUOTE_NONNUMERIC, lineterminator=\"\\n\")\n",
    "ASPECT4_LINES.to_csv(\"./outputs/export/honeyquest_lines.csv\", index=True, quoting=csv.QUOTE_NONNUMERIC, lineterminator=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌸 **Query Rating:** Hack and trap marks on a single query\n",
    "\n",
    "This is a _mark-based analysis_, meaning we report statistics per marks, i.e., users might place multiple marks within a single answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = [\n",
    "    \"TR849.filesystem.rowe.home-gaitan\",\n",
    "    \"TR849.filesystem.rowe.home-cooper\",\n",
    "]\n",
    "\n",
    "for query_id in query_ids:\n",
    "    display(query_id)\n",
    "    df = counting.get_query_rating(query_id, MARKS_DF, QUERIES_DICT)\n",
    "    plot_query_rating(df, f\"./outputs/queries/{query_id}.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📃 **LaTeX Generation:** Variables and tables for the final paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A4_NUM_MRK_HACK_SHORT = 40\n",
    "A4_NUM_MRK_HACK_LONG = 60  # was 50 for usenix\n",
    "A4_NUM_MRK_TRAP_SHORT = 15\n",
    "A4_NUM_MRK_TRAP_LONG = 60  # was 15 for usenix\n",
    "B1_MIN_BINOM_TEST_SAMPLES = 5\n",
    "\n",
    "store_latex_variables(\n",
    "    \"./outputs/variables.tex\",\n",
    "    {\n",
    "        \"TableLimitMrkHackShort\": A4_NUM_MRK_HACK_SHORT,\n",
    "        \"TableLimitMrkHackLong\": A4_NUM_MRK_HACK_LONG,\n",
    "        \"TableLimitMrkTrapShort\": A4_NUM_MRK_TRAP_SHORT,\n",
    "        \"TableLimitMrkTrapLong\": A4_NUM_MRK_TRAP_LONG,\n",
    "        \"AspectTwoMinimumSampleSize\": B1_MIN_BINOM_TEST_SAMPLES,\n",
    "        \"NumDroppedUsers\": NUM_DROPPED_USERS,\n",
    "        \"NumMinimumResponsesWithoutTutorial\": MIN_RESPONSES,\n",
    "    },\n",
    "    RESULTS_DICT,\n",
    "    MARKS_DF,\n",
    "    RESPONSES_DF,\n",
    "    RESPONSE_IDS,\n",
    "    USERS_DF,\n",
    "    QUERIES_DF,\n",
    "    QUERIES_DICT,\n",
    "    BUCKETS_DICT,\n",
    "    ID_DCPT_DF,\n",
    "    ID_RISK_DF,\n",
    "    ASPECT6A_DF,\n",
    "    ASPECT6B_DF,\n",
    "    ASPECT6C_DF,\n",
    "    CM_TRAP,\n",
    "    CM_HACK,\n",
    "    ASPECT1_DF,\n",
    "    ASPECT3_DCPT_DF,\n",
    "    ASPECT3_RISK_DF,\n",
    "    ASPECT2_DF,\n",
    "    ASPECT4_LINES,\n",
    ")\n",
    "\n",
    "for label in [\"deceptive\", \"risky\", \"neutral\"]:\n",
    "    generate_enticingness_latex_table(\n",
    "        f\"./outputs/tables/results-{label}.tex\",\n",
    "        QUERIES_DICT,\n",
    "        QUERIES_DF,\n",
    "        ASPECT1_DF,\n",
    "        ASPECT2_DF,\n",
    "        ASPECT3_DCPT_DF,\n",
    "        label,\n",
    "        min_test_samples=B1_MIN_BINOM_TEST_SAMPLES,\n",
    "    )\n",
    "\n",
    "generate_defensive_distraction_latex_table(\n",
    "    \"./outputs/tables/results-a2.tex\", ASPECT2_DF, ID_DCPT_DF\n",
    ")\n",
    "\n",
    "a4_sort_contents = [\n",
    "    ([\"mrk_hack\", \"mrk_trap\"], A4_NUM_MRK_HACK_LONG, True),\n",
    "    ([\"mrk_hack\", \"mrk_trap\"], A4_NUM_MRK_HACK_SHORT, False),\n",
    "    ([\"mrk_trap\", \"mrk_hack\"], A4_NUM_MRK_TRAP_LONG, True),\n",
    "    ([\"mrk_trap\", \"mrk_hack\"], A4_NUM_MRK_TRAP_SHORT, False),\n",
    "]\n",
    "\n",
    "for sort_by, limit, long in a4_sort_contents:\n",
    "    generate_mark_ranking_latex_table(\n",
    "        f\"./outputs/tables/results-a4-{sort_by[0].replace('_', '-')}{'-long' if long else '-short'}.tex\",\n",
    "        ASPECT4_LINES,\n",
    "        ID_DCPT_DF,\n",
    "        ID_RISK_DF,\n",
    "        sort_by=sort_by,\n",
    "        limit=limit,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honeyquest-36Gm5eJ_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "591b219f2d2663ba62a9c0a86fb534af7d704779d6016c984fe5cee549810e85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
